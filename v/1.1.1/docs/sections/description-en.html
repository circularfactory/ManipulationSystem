<h2 class="list" id="desc">Manipulation System Ontology: Description <span class="backlink"> back to <a href="#toc">ToC</a></span></h2>
<span class="markdown"><figure>
<img alt="" src="../Knowledgegraph-mani-system.drawio.svg"/><figcaption>knowledge graph of the manipulation system</figcaption>
</figure>
<h2 id="manipulator">Manipulator</h2>
<p>The Manipulator consists of the physical components and control logic to enable seamless execution of actions and data recording. The physical components of the manipulator encompass a RobotArm, equipped with a parallel jaw RobotGripper , which delivers proprioceptive data, including the joint configurations of the RobotArmState and the current GripperPose. To enable the capability of 6-DOF grasping, the GripperPose comprises Position and Orientation parameters, as well as the GripperAperture, i.e., the separation of the gripper fingers. To capture exteroceptive observations, i.e., information about the environment, the Manipulator is equipped with one or more RobotSensor s. A common example is a mounted RGB-D camera, which captures both depth and color information of the environment. These images are then transformed into structured SensorData, making it possible to trace where the data came from and to share it consistently across different systems. Enabled by the captured Data, the GraspLearningSystem generates physically grounded actions, either as a grasp pose or an entire trajectory. In the case of grasp pose prediction, a MotionPlanner computes a trajectory from the current GripperPose to the TargetGripperPose. Given the current configuration of the RobotArmState and the GripperPose, a control module executes the given trajectory. Subsequent changes to the environment are captured by the RobotSensors, establishing a closed-loop and interpretable data flow between the components of the Manipulator and the GraspLearningSystem. The developed ontology facilitates both modeling manipulators from RealWorldEnvironments, as well as SimulationEnvironments, as the basic components and control loop are similar in both.</p>
<p>Overall, this manipulator ontology systematically models the physical state of the robot, control logic, and perception history, thereby supporting task execution such as object picking, placement, and real-time adaptation in dynamic intralogistic environments. At the same time, we emphasize seamless integration with learning-based software components as a key contribution of this ontology, facilitating a structured information flow from sensor observations to neural inference and continuous lifelong learning.</p>
<h2 id="grasp-learning-system">Grasp Learning System</h2>
<p>The GraspLearningSystem is responsible for data processing, continual learning, inference, real-time decision making and action predictions. It leverages learned grasping strategies to infer adaptive grasp poses or trajectories, while also quantifying uncertainty in real-time.</p>
<p>SensorData is collected in a DataBuffer, where it may be pre-processed through labeling, filtering, weighting, or augmentation, depending on the concrete learning system. During training, this data is used by a TrainingSystem that trains NeuralNetworkModels. To ensure safety during training and evaluation, the GraspLearningSystem may employ a SimulationEnvironment, which mirrors the structure of the RealWorldEnvironment, ensuring seamless transfer between simulated and physical execution. During inference, the NeuralNetworkModule predicts gripper poses or trajectories based on current SensorData. The raw predictions are post-processed, e.g., through scaling or coordinate frame transformations, before being executed by the Manipulator. To support object-centric grasping, an ObjectDetector locates target components within a Workspace. The workspace acts both as a bounding box for perception and as a spatial constraint for manipulation, defining the Environment in which the manipulator operates. The proposed model is agnostic to a specific learning paradigms. It can be applied to open-loop grasp prediction, where the grasp poses are inferred from a single observation, as well as to closed-loop control strategies. By incorporating historical proprioceptive data into the control loop, the modeled system further supports lifelong learning. This is reinforced by the inclusion of a SimulationEnvironment, which allows for safe exploration during online learning.</p>
</span>